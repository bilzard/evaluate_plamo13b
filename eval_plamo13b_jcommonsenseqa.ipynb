{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JCommonsenseQA Full setによるPLaMo-13Bの評価を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"pfnet/plamo-13b\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"pfnet/plamo-13b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JCommonsenseQA - 1-shot\n",
    "\n",
    "PLaMo-13Bの性能を評価してみる。PFNの技術ブログ[1]によると、ベンチマークとしてJCommonsenseQAを用いたとのことなのでここでも同じものを用いる。\n",
    "\n",
    "### 評価方法\n",
    "\n",
    "まずは1-shotの評価を試みる。1-shotの性能評価では、質問と回答の形式を指定した例文をコンテクストとしてプロンプトに与えるのが一般的のようなので、ここでも同じ形式とする。\n",
    "\n",
    "### 評価指標 \n",
    "\n",
    "評価指標は[1]によるとnormalized accuracyが使われている。\n",
    "これは選択肢の順序の分布の不均衡の影響を減らすための指標で、以下のように定義される。\n",
    "\n",
    "$$\\text{norm\\_acc} = \\frac{1}{C} \\sum_{i=1}^{C} \\text{acc}_i$$\n",
    "\n",
    "### 参考資料\n",
    "\n",
    "- [1] https://tech.preferred.jp/ja/blog/llm-plamo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"leemeng/jcommonsenseqa-v1.1\", split=\"validation\")\n",
    "\n",
    "if N_SAMPLES is not None:\n",
    "    shuffled_dataset = dataset.shuffle(seed=42)\n",
    "    dataset = shuffled_dataset.select(range(N_SAMPLES))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 質問-回答例のサンプリング\n",
    "\n",
    "コンテクストとして与える質問と模範回答の例を作成する。モデルの評価にはvalidation splitを利用したので、ここではtrain splitからサンプリングする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"leemeng/jcommonsenseqa-v1.1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = \"\"\"質問: 主に子ども向けのもので、イラストのついた物語が書かれているものはどれ？\n",
    "choice0: 世界\n",
    "choice1: 写真集\n",
    "choice2: 絵本\n",
    "choice3: 論文\n",
    "choice4: 図鑑\n",
    "回答: 絵本\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in tqdm(enumerate(dataset), total=dataset.num_rows):\n",
    "    text = f\"\"\"質問: {item[\"question\"]}\\nchoice0: {item[\"choice0\"]}\\nchoice1: {item[\"choice1\"]}\\nchoice2: {item[\"choice2\"]}\\nchoice3: {item[\"choice3\"]}\\nchoice4: {item[\"choice4\"]}\\n解答: \"\"\"\n",
    "    prompt_text = f\"### 例 ###\\n{sample_prompt}\\n\\n{text}\"\n",
    "    print(prompt_text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for i, item in tqdm(enumerate(dataset), total=dataset.num_rows):\n",
    "    text = f\"\"\"質問: {item[\"question\"]}\\nchoice0: {item[\"choice0\"]}\\nchoice1: {item[\"choice1\"]}\\nchoice2: {item[\"choice2\"]}\\nchoice3: {item[\"choice3\"]}\\nchoice4: {item[\"choice4\"]}\\n解答: \"\"\"\n",
    "    prompt_text = f\"### 例 ###\\n{sample_prompt}\\n\\n{text}\"\n",
    "    prompt = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
    "    prompt_len = len(prompt[0])\n",
    "    prompt = prompt.to(model.device)\n",
    "    generated_tokens = model.generate(\n",
    "        inputs=prompt,\n",
    "        max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "    )[0]\n",
    "    generated_text = tokenizer.decode(generated_tokens[prompt_len:])\n",
    "    answer = generated_text.split(\"\\n\")[0]\n",
    "    answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers = []\n",
    "for item in dataset:\n",
    "    choices = [item[f\"choice{i}\"] for i in range(5)]\n",
    "    label = item[\"label\"]\n",
    "    correct_answers.append(choices[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = dataset.to_pandas()\n",
    "qa_df[\"answer\"] = answers\n",
    "qa_df[\"correct_answer\"] = correct_answers\n",
    "tag = f\"_sample{N_SAMPLES}\" if N_SAMPLES is not None else \"\"\n",
    "qa_df.to_csv(f\"jcommonsense_plamo13b{tag}.csv\")\n",
    "qa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価\n",
    "\n",
    "サンプルしたデータに対しては、約60%のnorm_accが得られた。PFNの公開データ[1]によると53.4%とのことなので、概ね似たスコアが得られている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def norm_acc(df):\n",
    "    df = df.copy()\n",
    "    accs = []\n",
    "    for label, df_ in df.groupby(\"label\"):\n",
    "        acc = df_.apply(lambda item: item[\"answer\"] == item[\"correct_answer\"], axis=1).mean()\n",
    "        accs.append(acc)\n",
    "    return np.mean(accs), accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, accs = norm_acc(qa_df)\n",
    "print(f\"norm_acc(1-shot): {100 * acc:.1f}%\")\n",
    "print([f\"{i}: {acc * 100:.1f}%\" for i, acc in enumerate(accs)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### A. 誤答について\n",
    "\n",
    "モデルが誤答した質問と回答について以下に示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = qa_df.query(\"answer != correct_answer\")\n",
    "\n",
    "for _, item in error_df.sample(min(100, len(error_df))).iterrows():\n",
    "    print(\n",
    "        f\"\"\"問題: {item[\"question\"]}\n",
    "選択肢: {item[\"choice0\"]}, {item[\"choice1\"]}, {item[\"choice2\"]}, {item[\"choice3\"]}, {item[\"choice4\"]}\n",
    "正答: {item[\"correct_answer\"]}\n",
    "モデルの回答: {item[\"answer\"]}\n",
    "\"\"\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
