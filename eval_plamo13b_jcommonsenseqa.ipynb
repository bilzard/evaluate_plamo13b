{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLaMo-13BのJCommonsenseQAによる評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 10\n",
    "# N_SAMPLES = None\n",
    "# full setでの評価には上記のコメントアウトを外す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"pfnet/plamo-13b\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"pfnet/plamo-13b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"leemeng/jcommonsenseqa-v1.1\", split=\"validation\")\n",
    "\n",
    "if N_SAMPLES is not None:\n",
    "    shuffled_dataset = dataset.shuffle(seed=42)\n",
    "    dataset = shuffled_dataset.select(range(N_SAMPLES))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 質問-回答例のサンプリング\n",
    "\n",
    "コンテクストとして与える質問と模範回答の例を作成する。モデルの評価にはvalidation splitを利用しているのでここではtrain splitからサンプリングする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"leemeng/jcommonsenseqa-v1.1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = \"\"\"質問: 主に子ども向けのもので、イラストのついた物語が書かれているものはどれ？\n",
    "choice0: 世界\n",
    "choice1: 写真集\n",
    "choice2: 絵本\n",
    "choice3: 論文\n",
    "choice4: 図鑑\n",
    "回答: 絵本\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in tqdm(enumerate(dataset), total=dataset.num_rows):\n",
    "    text = f\"\"\"質問: {item[\"question\"]}\\nchoice0: {item[\"choice0\"]}\\nchoice1: {item[\"choice1\"]}\\nchoice2: {item[\"choice2\"]}\\nchoice3: {item[\"choice3\"]}\\nchoice4: {item[\"choice4\"]}\\n解答: \"\"\"\n",
    "    prompt_text = f\"### 例 ###\\n{sample_prompt}\\n\\n{text}\"\n",
    "    print(prompt_text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for i, item in tqdm(enumerate(dataset), total=dataset.num_rows):\n",
    "    text = f\"\"\"質問: {item[\"question\"]}\\nchoice0: {item[\"choice0\"]}\\nchoice1: {item[\"choice1\"]}\\nchoice2: {item[\"choice2\"]}\\nchoice3: {item[\"choice3\"]}\\nchoice4: {item[\"choice4\"]}\\n解答: \"\"\"\n",
    "    prompt_text = f\"### 例 ###\\n{sample_prompt}\\n\\n{text}\"\n",
    "    prompt = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
    "    prompt_len = len(prompt[0])\n",
    "    prompt = prompt.to(model.device)\n",
    "    generated_tokens = model.generate(\n",
    "        inputs=prompt,\n",
    "        max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "    )[0]\n",
    "    generated_text = tokenizer.decode(generated_tokens[prompt_len:])\n",
    "    answer = generated_text.split(\"\\n\")[0]\n",
    "    answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers = []\n",
    "for item in dataset:\n",
    "    choices = [item[f\"choice{i}\"] for i in range(5)]\n",
    "    label = item[\"label\"]\n",
    "    correct_answers.append(choices[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = dataset.to_pandas()\n",
    "qa_df[\"answer\"] = answers\n",
    "qa_df[\"correct_answer\"] = correct_answers\n",
    "tag = f\"_sample{N_SAMPLES}\" if N_SAMPLES is not None else \"\"\n",
    "qa_df.to_csv(f\"jcommonsense_plamo13b{tag}.csv\")\n",
    "qa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_acc(df):\n",
    "    df = df.copy()\n",
    "    accs = []\n",
    "    for label, df_ in df.groupby(\"label\"):\n",
    "        acc = df_.apply(lambda item: item[\"answer\"] == item[\"correct_answer\"], axis=1).mean()\n",
    "        accs.append(acc)\n",
    "    return np.mean(accs), accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, accs = norm_acc(qa_df)\n",
    "print(f\"norm_acc(1-shot): {100 * acc:.1f}%\")\n",
    "print([f\"{i}: {acc * 100:.1f}%\" for i, acc in enumerate(accs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤答の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = qa_df.query(\"answer != correct_answer\")\n",
    "\n",
    "for _, item in error_df.sample(min(100, len(error_df))).iterrows():\n",
    "    print(\n",
    "        f\"\"\"問題: {item[\"question\"]}\n",
    "選択肢: {item[\"choice0\"]}, {item[\"choice1\"]}, {item[\"choice2\"]}, {item[\"choice3\"]}, {item[\"choice4\"]}\n",
    "正答: {item[\"correct_answer\"]}\n",
    "モデルの回答: {item[\"answer\"]}\n",
    "\"\"\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
